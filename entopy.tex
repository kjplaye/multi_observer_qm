\subsubsection{Classical Entropy}
We recall Shannon's classical entropy of $P$
\[
 H := -\sum p_i \log(p_i)
\]
and relative entropy, or Kullbackâ€“Leibler divergence, of a model $R$ under $P$
\[
D_{KL}(P||R) := -\sum p_i \log{\frac{r_i}{p_i}}.
\]
$H$ is the average amount of ``surprize'' when using $P$, or the expected $P$ information
\[
H = \mathbb{E}[-\log{P}].
\] 
For relative entropy we have another expected value, the cross entropy, or expected $R$ information $\mathbb{E}[-\log{R}]$.  We see how these are related in
\[
D_{KL}(P||R) + H(P) = \mathbb{E}[-\log{R}]  \ge H(P)
\]
where the last inequality is Gibbs' inequality.  It is equality exactly when $R$ = $P$.

